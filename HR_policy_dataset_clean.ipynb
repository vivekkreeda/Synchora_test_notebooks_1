{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMrP7cH0CANE",
        "outputId": "5b3d697c-e45f-4100-a8b7-db1c15159d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top keys seen: [('messages', 644)]\n",
            "Valid pairs: 644\n",
            "Wrote cleaned JSONL to: /content/qa_clean_hr.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Normalize /mnt/data/HR_policy.jsonl -> /content/qa_clean_hr.jsonl\n",
        "# Supports {\"user\",\"assistant\"} OR {\"messages\":[{\"role\":\"user|assistant\",\"content\":...}, ...]}\n",
        "\n",
        "import json, re\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "IN_JSONL  = \"/content/HR_policy.jsonl\"   # <- change if needed\n",
        "OUT_JSONL = \"/content/qa_clean_hr.jsonl\"\n",
        "Path(OUT_JSONL).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_from_messages(obj):\n",
        "    msgs = obj.get(\"messages\")\n",
        "    if not isinstance(msgs, list) or not msgs:\n",
        "        return None, None\n",
        "    # last user + first assistant after it\n",
        "    user_text = None\n",
        "    for m in reversed(msgs):\n",
        "        if isinstance(m, dict) and m.get(\"role\",\"\").lower() == \"user\":\n",
        "            user_text = str(m.get(\"content\",\"\")).strip()\n",
        "            break\n",
        "    if not user_text:\n",
        "        return None, None\n",
        "    seen_user = False\n",
        "    for m in msgs:\n",
        "        if isinstance(m, dict) and m.get(\"role\",\"\").lower() == \"user\" and str(m.get(\"content\",\"\")).strip() == user_text:\n",
        "            seen_user = True\n",
        "            continue\n",
        "        if seen_user and isinstance(m, dict) and m.get(\"role\",\"\").lower() == \"assistant\":\n",
        "            return user_text, str(m.get(\"content\",\"\")).strip()\n",
        "    return user_text, None\n",
        "\n",
        "def extract_qa(obj):\n",
        "    # 1) messages schema\n",
        "    u, a = get_from_messages(obj)\n",
        "    if u and a: return u, a\n",
        "    # 2) direct key variants\n",
        "    for uk, ak in [\n",
        "        (\"user\",\"assistant\"), (\"question\",\"answer\"),\n",
        "        (\"input\",\"output\"), (\"prompt\",\"response\"),\n",
        "        (\"instruction\",\"output\"), (\"human\",\"bot\"),\n",
        "        (\"User\",\"Assistant\")\n",
        "    ]:\n",
        "        u = str(obj.get(uk,\"\") or \"\").strip()\n",
        "        a = str(obj.get(ak,\"\") or \"\").strip()\n",
        "        if u and a: return u, a\n",
        "    # 3) free-text tagged\n",
        "    t = str(obj.get(\"text\",\"\") or \"\").strip()\n",
        "    if t:\n",
        "        m = re.search(r\"User:\\s*(.+?)\\s*Assistant:\\s*(.+)\", t, flags=re.S|re.I)\n",
        "        if m: return m.group(1).strip(), m.group(2).strip()\n",
        "    return None, None\n",
        "\n",
        "pairs, key_counter = [], Counter()\n",
        "with open(IN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except Exception:\n",
        "            continue\n",
        "        key_counter.update(obj.keys())\n",
        "        u, a = extract_qa(obj)\n",
        "        if u and a:\n",
        "            pairs.append({\"prompt\": f\"User: {u}\\nAssistant:\", \"completion\": a})\n",
        "\n",
        "print(\"Top keys seen:\", key_counter.most_common(20))\n",
        "print(\"Valid pairs:\", len(pairs))\n",
        "if not pairs:\n",
        "    raise ValueError(\"No Q/A pairs found. Please show a couple of raw lines so we can adjust the parser.\")\n",
        "\n",
        "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as w:\n",
        "    for r in pairs:\n",
        "        w.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote cleaned JSONL to:\", OUT_JSONL)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA SFT for HR policy Q&A \u2014 plain Transformers + PEFT (version-friendly)\n",
        "\n",
        "!pip -q install \"transformers>=4.31.0\" \"datasets>=2.14.0\" \"peft>=0.6.0\" accelerate\n",
        "\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_JSONL = \"/content/qa_clean_hr.jsonl\"    # from step 1\n",
        "MODEL_NAME = \"facebook/opt-350m\"             # start small; you can switch to opt-1.3b later\n",
        "OUTPUT_DIR = \"/content/hr_lora_out\"\n",
        "MAX_LEN = 1024\n",
        "BATCH = 2\n",
        "EPOCHS = 5\n",
        "LR = 2e-4\n",
        "\n",
        "# --- Load & split ---\n",
        "ds = load_dataset(\"json\", data_files=DATA_JSONL, split=\"train\")\n",
        "splits = ds.train_test_split(test_size=0.1, seed=SEED)\n",
        "train_ds, val_ds = splits[\"train\"], splits[\"test\"]\n",
        "\n",
        "# --- Tokenizer & base ---\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Apply LoRA (OPT-style) ---\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"]\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "# --- Dataset: mask prompt so loss is only on completion ---\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, hf_split, tokenizer: AutoTokenizer, max_len: int):\n",
        "        self.data = hf_split; self.tok = tokenizer; self.max_len = max_len\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        prompt = ex[\"prompt\"]         # \"User: ...\\nAssistant:\"\n",
        "        completion = ex[\"completion\"] # assistant answer\n",
        "        prompt_ids = self.tok(prompt, add_special_tokens=False)[\"input_ids\"]\n",
        "        completion_ids = self.tok(\" \" + completion, add_special_tokens=False)[\"input_ids\"]\n",
        "        input_ids = prompt_ids + completion_ids + [self.tok.eos_token_id]\n",
        "        labels    = [-100]*len(prompt_ids) + completion_ids + [self.tok.eos_token_id]\n",
        "        if len(input_ids) > self.max_len:\n",
        "            input_ids = input_ids[-self.max_len:]\n",
        "            labels    = labels[-self.max_len:]\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_torch = ChatDataset(train_ds, tok, MAX_LEN)\n",
        "val_torch   = ChatDataset(val_ds, tok, MAX_LEN)\n",
        "\n",
        "# --- Simple padding collator ---\n",
        "@dataclass\n",
        "class PadCollator:\n",
        "    pad_id: int\n",
        "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
        "        def pad_vec(v, pad_val):\n",
        "            pad_len = maxlen - v.size(0)\n",
        "            return torch.cat([v, torch.full((pad_len,), pad_val, dtype=v.dtype)], dim=0) if pad_len>0 else v\n",
        "        input_ids = torch.stack([pad_vec(x[\"input_ids\"], self.pad_id) for x in batch])\n",
        "        attention = torch.stack([pad_vec(x[\"attention_mask\"], 0) for x in batch])\n",
        "        labels    = torch.stack([pad_vec(x[\"labels\"], -100) for x in batch])\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention, \"labels\": labels}\n",
        "\n",
        "collator = PadCollator(pad_id=tok.pad_token_id)\n",
        "\n",
        "# --- Train (no evaluation_strategy) ---\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=collator,\n",
        "    train_dataset=train_torch,\n",
        "    eval_dataset=val_torch,  # we\u2019ll evaluate manually after training\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Eval metrics:\", metrics)\n",
        "\n",
        "# --- Save adapters & tokenizer ---\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tok.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved LoRA adapters to:\", OUTPUT_DIR)\n",
        "\n",
        "# --- Inference helper ---\n",
        "from peft import PeftModel\n",
        "def hr_chat(user_text, max_new_tokens=200, mode=\"greedy\"):\n",
        "    base = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    peft = PeftModel.from_pretrained(base, OUTPUT_DIR).eval()\n",
        "    system_hint = (\"You are an HR information assistant. Provide general policy info, \"\n",
        "                   \"not legal advice. Refer to HR best practices and encourage checking company policy.\\n\")\n",
        "    prompt = f\"{system_hint}User: {user_text}\\nAssistant:\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(peft.device)\n",
        "    gen_kwargs = dict(max_new_tokens=max_new_tokens)\n",
        "    if mode == \"greedy\":\n",
        "        gen_kwargs.update(dict(do_sample=False))\n",
        "    else:\n",
        "        gen_kwargs.update(dict(do_sample=True, temperature=0.7, top_p=0.9))\n",
        "    with torch.no_grad():\n",
        "        out = peft.generate(**inputs, **gen_kwargs)\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "print(hr_chat(\"What\u2019s a typical PTO policy structure?\", mode=\"topp\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "KaKSUC-UDpCa",
        "outputId": "0bd35313-ca45-42b1-f6a4-7bb763b81f2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/Dtype.cpp\":177, please report a bug to PyTorch. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3136123913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/Dtype.cpp\":177, please report a bug to PyTorch. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio xformers\n",
        "!pip cache purge\n",
        "\n",
        "# GPU version (CUDA 12.1, typical for Colab)\n",
        "!pip install --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n",
        "\n",
        "# Optional (speedups on some GPUs)\n",
        "!pip install xformers==0.0.27.post2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp4wLxTCEE23",
        "outputId": "eb143258-540f-42ac-b0de-34ae30e42ab3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (798.9 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.1) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.1) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0\n",
            "Collecting xformers==0.0.27.post2\n",
            "  Downloading xformers-0.0.27.post2-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers==0.0.27.post2) (2.0.2)\n",
            "Collecting torch==2.4.0 (from xformers==0.0.27.post2)\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->xformers==0.0.27.post2) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->xformers==0.0.27.post2) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0->xformers==0.0.27.post2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0->xformers==0.0.27.post2) (1.3.0)\n",
            "Downloading xformers-0.0.27.post2-cp312-cp312-manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.4.0 xformers-0.0.27.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, torchaudio, platform\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"TorchVision:\", torchvision.__version__)\n",
        "print(\"TorchAudio:\", torchaudio.__version__)\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLkgHEzQEKVu",
        "outputId": "b23d8f97-f911-460c-ebdf-4bddb5e521f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.4.0+cu121\n",
            "TorchVision: 0.19.1+cu121\n",
            "TorchAudio: 2.4.1+cu121\n",
            "Python: 3.12.11\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA SFT for HR Q&A \u2014 plain Transformers + PEFT (version-friendly; safetensors base)\n",
        "\n",
        "import random, os\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_JSONL = \"/content/qa_clean_hr.jsonl\"    # from step 1\n",
        "MODEL_NAME = \"facebook/opt-1.3b\"             # ships safetensors (avoids torch<2.6 restriction)\n",
        "OUTPUT_DIR = \"/content/hr_lora_out\"\n",
        "MAX_LEN = 1024\n",
        "BATCH = 2\n",
        "EPOCHS = 5\n",
        "LR = 2e-4\n",
        "\n",
        "# --- Load & split ---\n",
        "ds = load_dataset(\"json\", data_files=DATA_JSONL, split=\"train\")\n",
        "splits = ds.train_test_split(test_size=0.1, seed=SEED)\n",
        "train_ds, val_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Train/Val sizes:\", len(train_ds), len(val_ds))\n",
        "\n",
        "# --- Tokenizer & base ---\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "# Force safetensors load to avoid CVE guard on torch<2.6\n",
        "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
        "\n",
        "# --- Apply LoRA (OPT-style targets; adjust if you change base arch) ---\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"]\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "# --- Dataset: mask prompt so loss is only on completion ---\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, hf_split, tokenizer: AutoTokenizer, max_len: int):\n",
        "        self.data = hf_split; self.tok = tokenizer; self.max_len = max_len\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.data[idx]\n",
        "        prompt = ex[\"prompt\"]          # \"User: ...\\nAssistant:\"\n",
        "        completion = ex[\"completion\"]  # assistant answer\n",
        "        prompt_ids = self.tok(prompt, add_special_tokens=False)[\"input_ids\"]\n",
        "        completion_ids = self.tok(\" \" + completion, add_special_tokens=False)[\"input_ids\"]\n",
        "        input_ids = prompt_ids + completion_ids + [self.tok.eos_token_id]\n",
        "        labels    = [-100]*len(prompt_ids) + completion_ids + [self.tok.eos_token_id]\n",
        "        if len(input_ids) > self.max_len:\n",
        "            input_ids = input_ids[-self.max_len:]\n",
        "            labels    = labels[-self.max_len:]\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_torch = ChatDataset(train_ds, tok, MAX_LEN)\n",
        "val_torch   = ChatDataset(val_ds, tok, MAX_LEN)\n",
        "\n",
        "# --- Simple padding collator ---\n",
        "@dataclass\n",
        "class PadCollator:\n",
        "    pad_id: int\n",
        "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        maxlen = max(x[\"input_ids\"].size(0) for x in batch)\n",
        "        def pad_vec(v, pad_val):\n",
        "            pad_len = maxlen - v.size(0)\n",
        "            return torch.cat([v, torch.full((pad_len,), pad_val, dtype=v.dtype)], dim=0) if pad_len>0 else v\n",
        "        input_ids = torch.stack([pad_vec(x[\"input_ids\"], self.pad_id) for x in batch])\n",
        "        attention = torch.stack([pad_vec(x[\"attention_mask\"], 0) for x in batch])\n",
        "        labels    = torch.stack([pad_vec(x[\"labels\"], -100) for x in batch])\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention, \"labels\": labels}\n",
        "\n",
        "collator = PadCollator(pad_id=tok.pad_token_id)\n",
        "\n",
        "# --- Training (no evaluation_strategy; evaluate after) ---\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=collator,\n",
        "    train_dataset=train_torch,\n",
        "    eval_dataset=val_torch,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Eval metrics:\", metrics)\n",
        "\n",
        "# --- Save adapters & tokenizer ---\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tok.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved LoRA adapters to:\", OUTPUT_DIR)\n",
        "\n",
        "# --- Inference helper ---\n",
        "def hr_chat(user_text, max_new_tokens=200, mode=\"greedy\"):\n",
        "    base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
        "    peft = PeftModel.from_pretrained(base, OUTPUT_DIR).eval()\n",
        "    system_hint = (\"You are an HR information assistant. Provide general policy info, \"\n",
        "                   \"not legal advice. Encourage checking the company handbook.\\n\")\n",
        "    prompt = f\"{system_hint}User: {user_text}\\nAssistant:\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(peft.device)\n",
        "    gen_kwargs = dict(max_new_tokens=max_new_tokens)\n",
        "    if mode == \"greedy\":\n",
        "        gen_kwargs.update(dict(do_sample=False))\n",
        "    else:\n",
        "        gen_kwargs.update(dict(do_sample=True, temperature=0.7, top_p=0.9))\n",
        "    with torch.no_grad():\n",
        "        out = peft.generate(**inputs, **gen_kwargs)\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "print(hr_chat(\"What are common components of a PTO policy?\", mode=\"topp\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "84a375a8b74248fe9e2816fdedf710de",
            "0e26d4639986451bb7538aa2d6df5840",
            "0b596831afb146e2b5f2b0f347c7c2b6",
            "13ba649363534654b77190e09696b0eb",
            "b72cf5fdee1440378d35ca98ca213d66",
            "1330d156de2343dbacfd0baca0cb3345",
            "90f24b0cf7e74862bfa8e62652fa3836",
            "d026ad6f38984c049e1ba11886818dbb",
            "43adbf99335c4a658a7f4624365d0607",
            "5bcaaac9d7cb483191c3488343537ab0",
            "736d58ff4e73426db546164e251dc4d0",
            "64350d1f593b4fbdb0fcfd2b6302a8f0",
            "404ea3572a65422888fd7f26ae7cd41b",
            "95fb840d04e14566b3245e00e1a541f1",
            "fccb97c51d914ad3aab9a7c445b56185",
            "2288af0c148549819808d0d0cc6275e5",
            "a6abfdf876e44709869c8796938f3050",
            "19559b37dabd4edfbdd7fc69ed38a92d",
            "b7982694e9004e74a953a26fe9b7084f",
            "2cd32b5bb60547caa96fee62ed735211",
            "90154770fa874e26aa0d2f8eba6df2e2",
            "7905729b5369449394065b758d520375",
            "ac4fcb5c10274034a7a48c1878779656",
            "b963c1a3c9954869b8979a4477c93fd1",
            "8e05fd6cad9442c5a30cf511c8b7a890",
            "b49d2a4058dd4086b76dc524942de960",
            "2daa783abcf846b7840898f563cacc55",
            "01935f8bb275436bb29d139927258982",
            "8412f1731801410b94969e408266693e",
            "14de283d78a54da883b5090abce37e24",
            "1a3ecdec9f344f15a4a6936c9412bd61",
            "d521b791bb284405be0be8449e917081",
            "c7e0b7747c0a43b78b59534033253cca",
            "67a9992f38cc4c7b9fcec9d8aac202e3",
            "ebdef4b5492f4fc3859f841466812122",
            "fbac41e59e1346b9ad9cd27837056b2f",
            "39f9b353ba9e4bc09e1e3a9d732b25d8",
            "63c183a4c1b8430cba8e6d9555831668",
            "098e05a69ced47e2ae39a9d1ad003006",
            "d739f41d2600498987a8d33a131af76d",
            "f1a07faef6af40ea84d5027965a4ae6a",
            "3b87f107fa1d4d85a3c06c1599642594",
            "3b24860d7c0746bfa46f0f25cfe37de5",
            "955a85fc041447d38680ec89bd12a734",
            "5063e0c9258b4076bf3db6b6fbb783a7",
            "d3c667c10f78490ebec1e41a83740bf3",
            "59afc3e3374b42d0b57146571e515eb7",
            "d5d2e37fe2d141c5aa90a6440d684922",
            "935fbb69e60540898bc742dc96941344",
            "0eabd24de75a4f83a7cd404dcf51ca04",
            "ad500ef122ce4d3a9f3664bc34cff7d1",
            "61ed07b385614fefa879b0ddb69f6982",
            "ce088dc41b074559a3a1e630491d43a1",
            "c4637029e0dd4b99a2ae9ec9fc4dc3a8",
            "d80d5c9d71574bb78609df30f93ede89",
            "d0ae6dd24a04401a8097a5c7f2bbb26a",
            "7b8c5b8b305240b4aa455653032e3c49",
            "6635137b9b064e74b984f66a4b0b9b2e",
            "bf07be656ee94e35a2ec1ec57d7ba416",
            "c9d0a333176343ad99bc38507530828d",
            "224a694487cf4ccabafca3b732eff57b",
            "08bef47288cb4f9fbce10dc6123003bb",
            "8205190f06434a87addca0df9bafe1db",
            "40c6875808c949c093d6ef32be32bfc5",
            "15cd00aa7fe04812a2d1fabe1becf164",
            "46c2bd80590b4956b256848e72cf0d77",
            "f1cac6134e6146c8a13721493ac9196d",
            "fa154b61fe134b05a2169774f4e240fb",
            "801433ab1ab74c6382006ec6734e5b38",
            "669d23ad741646fea870de6982d3faff",
            "50bd40e2d15b42c3bc22ab50cac202e8",
            "797f0544b2b945e3b1adc8d38dd6db82",
            "c31f689a072f41b7bff17a648161e3ef",
            "e88eeee4515b4f8f99085897b8099d16",
            "45fc3509ef0c4208b8103db667dded52",
            "4adc12f970e44fe4a12734c637e6262b",
            "415a7dae885547dba2e82cff5d21425f",
            "f9e5721e42c749b3a1f453d27fcf3b89",
            "e140c3d14d9b4f08a34c4881fba9a22e",
            "a2eca60173db406eac7ee37a0ace92a6",
            "d55d3b0869f74342b63c4f6fc3a88402",
            "5c436eede7314fb3b180e380aafe1aad",
            "200c582c5e4f4e3cbe0889a201325fc7",
            "08ec276fbe0c4a02a1b654e7dd2f06e9",
            "5d5541b813a4494687c0bd378a75f5ae",
            "5316cddfe65c45f9b66c40235c11383e",
            "03b1e163cb994bacbe2cf2aaed1f512c",
            "ec1c9391b26e4f45917ebad8f5d1a4b8"
          ]
        },
        "id": "UDY2xv-WFlWh",
        "outputId": "7ea33cc7-461a-4202-fac8-b965b5a4d029"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 579 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1450' max='1450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1450/1450 04:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.827400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.627300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.433700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.466800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.488800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.434600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.419600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.172900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.053900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.073000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.150700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.065400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.080900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.146800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.185700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.952800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.103000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.785700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.785000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.784600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.756100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.744900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.874900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.742000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.702200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.713500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.888400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.483900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.478100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.519900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.516800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.667400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.510100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.602900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.589200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.421200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.365200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.428900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.421400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.346900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.388500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.361800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.413200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.434700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>0.403600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.438000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33/33 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval metrics: {'eval_loss': 1.450061559677124, 'eval_runtime': 2.3333, 'eval_samples_per_second': 27.858, 'eval_steps_per_second': 14.143, 'epoch': 5.0}\n",
            "Saved LoRA adapters to: /content/hr_lora_out\n",
            "Common components of a PTO policy include employee leave arrangements, PTO eligibility, reporting procedures, and the specific time limits for accrual and use. Keep these details updated to ensure everyone understands and follows the policy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token length histograms (helps choose MAX_LEN)\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATA_JSONL = \"/content/qa_clean_hr.jsonl\"\n",
        "BASE = \"facebook/opt-1.3b\"\n",
        "tok = AutoTokenizer.from_pretrained(BASE, use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=DATA_JSONL, split=\"train\")\n",
        "def tlen(s): return len(tok(s, add_special_tokens=False)[\"input_ids\"])\n",
        "p_l = [tlen(r[\"prompt\"]) for r in ds]\n",
        "c_l = [tlen(r[\"completion\"]) for r in ds]\n",
        "\n",
        "plt.figure(figsize=(6,4)); plt.hist(p_l, bins=40); plt.title(\"Prompt token lengths\"); plt.show()\n",
        "plt.figure(figsize=(6,4)); plt.hist(c_l, bins=40); plt.title(\"Completion token lengths\"); plt.show()\n",
        "\n",
        "print(\"Prompt mean/p95:\", round(np.mean(p_l),1), round(np.percentile(p_l,95),1))\n",
        "print(\"Completion mean/p95:\", round(np.mean(c_l),1), round(np.percentile(c_l,95),1))\n",
        "print(\"Total p95:\", round(np.percentile([a+b for a,b in zip(p_l,c_l)],95),1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "KFGXFq0fGnPh",
        "outputId": "0cfcbd4b-a76a-4971-d3b9-97f9a9d4c0de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt mean/p95: 19.8 27.0\n",
            "Completion mean/p95: 41.9 68.9\n",
            "Total p95: 91.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(hr_chat(\"How often is the company policy reviewed and updated?\", mode=\"topp\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Yz8FWDIKA7",
        "outputId": "94274179-3a02-4661-f015-407357a84d2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The company policy is reviewed annually and updated as necessary to maintain compliance with current laws and industry standards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhK1decXQHRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}